{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "validation": [
      {
        "accuracy": 0.709493,
        "f1": 0.666246,
        "f1_weighted": 0.71024,
        "scores_per_experiment": [
          {
            "accuracy": 0.713724,
            "f1": 0.675656,
            "f1_weighted": 0.715527
          },
          {
            "accuracy": 0.714707,
            "f1": 0.657407,
            "f1_weighted": 0.718548
          },
          {
            "accuracy": 0.699459,
            "f1": 0.649299,
            "f1_weighted": 0.698877
          },
          {
            "accuracy": 0.731431,
            "f1": 0.680162,
            "f1_weighted": 0.733534
          },
          {
            "accuracy": 0.717659,
            "f1": 0.678374,
            "f1_weighted": 0.716104
          },
          {
            "accuracy": 0.706345,
            "f1": 0.660983,
            "f1_weighted": 0.70172
          },
          {
            "accuracy": 0.722577,
            "f1": 0.683174,
            "f1_weighted": 0.722685
          },
          {
            "accuracy": 0.681751,
            "f1": 0.649071,
            "f1_weighted": 0.680915
          },
          {
            "accuracy": 0.708805,
            "f1": 0.662,
            "f1_weighted": 0.715648
          },
          {
            "accuracy": 0.698475,
            "f1": 0.66633,
            "f1_weighted": 0.698837
          }
        ],
        "main_score": 0.709493,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.713584,
        "f1": 0.676591,
        "f1_weighted": 0.71531,
        "scores_per_experiment": [
          {
            "accuracy": 0.719233,
            "f1": 0.691567,
            "f1_weighted": 0.722285
          },
          {
            "accuracy": 0.721251,
            "f1": 0.673759,
            "f1_weighted": 0.724726
          },
          {
            "accuracy": 0.70074,
            "f1": 0.674827,
            "f1_weighted": 0.698105
          },
          {
            "accuracy": 0.733692,
            "f1": 0.686491,
            "f1_weighted": 0.736985
          },
          {
            "accuracy": 0.710491,
            "f1": 0.680731,
            "f1_weighted": 0.706893
          },
          {
            "accuracy": 0.702421,
            "f1": 0.662968,
            "f1_weighted": 0.70054
          },
          {
            "accuracy": 0.729657,
            "f1": 0.685312,
            "f1_weighted": 0.733855
          },
          {
            "accuracy": 0.697377,
            "f1": 0.663008,
            "f1_weighted": 0.699695
          },
          {
            "accuracy": 0.717552,
            "f1": 0.672198,
            "f1_weighted": 0.724073
          },
          {
            "accuracy": 0.70343,
            "f1": 0.675051,
            "f1_weighted": 0.705949
          }
        ],
        "main_score": 0.713584,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 24.626436948776245,
  "kg_co2_emissions": null
}