{
  "dataset_revision": "3c22f7e6bf4e366c86d69293c9164bf9e9d80aac",
  "task_name": "SynPerChatbotConvSAFear",
  "mteb_version": "1.34.14",
  "scores": {
    "test": [
      {
        "accuracy": 0.897436,
        "f1": 0.893855,
        "f1_weighted": 0.897974,
        "ap": 0.901701,
        "ap_weighted": 0.901701,
        "scores_per_experiment": [
          {
            "accuracy": 0.905983,
            "f1": 0.902537,
            "f1_weighted": 0.906453,
            "ap": 0.908213,
            "ap_weighted": 0.908213
          },
          {
            "accuracy": 0.897436,
            "f1": 0.893313,
            "f1_weighted": 0.897794,
            "ap": 0.895918,
            "ap_weighted": 0.895918
          },
          {
            "accuracy": 0.888889,
            "f1": 0.884008,
            "f1_weighted": 0.889092,
            "ap": 0.883974,
            "ap_weighted": 0.883974
          },
          {
            "accuracy": 0.91453,
            "f1": 0.911685,
            "f1_weighted": 0.915072,
            "ap": 0.920876,
            "ap_weighted": 0.920876
          },
          {
            "accuracy": 0.880342,
            "f1": 0.876359,
            "f1_weighted": 0.881101,
            "ap": 0.884995,
            "ap_weighted": 0.884995
          },
          {
            "accuracy": 0.91453,
            "f1": 0.912215,
            "f1_weighted": 0.915261,
            "ap": 0.928398,
            "ap_weighted": 0.928398
          },
          {
            "accuracy": 0.888889,
            "f1": 0.885544,
            "f1_weighted": 0.889725,
            "ap": 0.897239,
            "ap_weighted": 0.897239
          },
          {
            "accuracy": 0.923077,
            "f1": 0.920257,
            "f1_weighted": 0.923461,
            "ap": 0.926386,
            "ap_weighted": 0.926386
          },
          {
            "accuracy": 0.897436,
            "f1": 0.891667,
            "f1_weighted": 0.897009,
            "ap": 0.883175,
            "ap_weighted": 0.883175
          },
          {
            "accuracy": 0.863248,
            "f1": 0.860963,
            "f1_weighted": 0.864771,
            "ap": 0.887837,
            "ap_weighted": 0.887837
          }
        ],
        "main_score": 0.897436,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 10.329763650894165,
  "kg_co2_emissions": null
}