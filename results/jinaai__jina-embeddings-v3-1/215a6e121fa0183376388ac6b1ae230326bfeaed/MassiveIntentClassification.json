{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.25.8",
  "scores": {
    "validation": [
      {
        "accuracy": 0.734924,
        "f1": 0.672803,
        "f1_weighted": 0.716875,
        "scores_per_experiment": [
          {
            "accuracy": 0.737826,
            "f1": 0.672335,
            "f1_weighted": 0.720739
          },
          {
            "accuracy": 0.761436,
            "f1": 0.683376,
            "f1_weighted": 0.743028
          },
          {
            "accuracy": 0.747172,
            "f1": 0.691352,
            "f1_weighted": 0.730026
          },
          {
            "accuracy": 0.73635,
            "f1": 0.664457,
            "f1_weighted": 0.71743
          },
          {
            "accuracy": 0.743237,
            "f1": 0.67513,
            "f1_weighted": 0.7276
          },
          {
            "accuracy": 0.714707,
            "f1": 0.651524,
            "f1_weighted": 0.699161
          },
          {
            "accuracy": 0.749631,
            "f1": 0.691661,
            "f1_weighted": 0.731549
          },
          {
            "accuracy": 0.713724,
            "f1": 0.64538,
            "f1_weighted": 0.691666
          },
          {
            "accuracy": 0.709788,
            "f1": 0.671022,
            "f1_weighted": 0.685785
          },
          {
            "accuracy": 0.735366,
            "f1": 0.681797,
            "f1_weighted": 0.721768
          }
        ],
        "main_score": 0.734924,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.725757,
        "f1": 0.67305,
        "f1_weighted": 0.709762,
        "scores_per_experiment": [
          {
            "accuracy": 0.737727,
            "f1": 0.673435,
            "f1_weighted": 0.7214
          },
          {
            "accuracy": 0.745797,
            "f1": 0.688249,
            "f1_weighted": 0.726297
          },
          {
            "accuracy": 0.734028,
            "f1": 0.683279,
            "f1_weighted": 0.71806
          },
          {
            "accuracy": 0.725286,
            "f1": 0.663943,
            "f1_weighted": 0.709781
          },
          {
            "accuracy": 0.726631,
            "f1": 0.678679,
            "f1_weighted": 0.711883
          },
          {
            "accuracy": 0.707801,
            "f1": 0.650394,
            "f1_weighted": 0.694659
          },
          {
            "accuracy": 0.741426,
            "f1": 0.695531,
            "f1_weighted": 0.726524
          },
          {
            "accuracy": 0.718225,
            "f1": 0.649614,
            "f1_weighted": 0.699588
          },
          {
            "accuracy": 0.711836,
            "f1": 0.678732,
            "f1_weighted": 0.691247
          },
          {
            "accuracy": 0.70881,
            "f1": 0.668645,
            "f1_weighted": 0.698177
          }
        ],
        "main_score": 0.725757,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 140.71828055381775,
  "kg_co2_emissions": null
}