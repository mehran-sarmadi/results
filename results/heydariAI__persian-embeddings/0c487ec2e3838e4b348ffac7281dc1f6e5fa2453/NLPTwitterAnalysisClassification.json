{
  "dataset_revision": "4ceb1312583fd2c7c73ad2d550b726124dcd39a0",
  "task_name": "NLPTwitterAnalysisClassification",
  "mteb_version": "1.38.34",
  "scores": {
    "test": [
      {
        "accuracy": 0.772794,
        "f1": 0.790637,
        "f1_weighted": 0.773769,
        "scores_per_experiment": [
          {
            "accuracy": 0.770588,
            "f1": 0.787324,
            "f1_weighted": 0.771493
          },
          {
            "accuracy": 0.769853,
            "f1": 0.788075,
            "f1_weighted": 0.769767
          },
          {
            "accuracy": 0.766176,
            "f1": 0.785848,
            "f1_weighted": 0.76554
          },
          {
            "accuracy": 0.769118,
            "f1": 0.785774,
            "f1_weighted": 0.768439
          },
          {
            "accuracy": 0.778676,
            "f1": 0.79777,
            "f1_weighted": 0.78178
          },
          {
            "accuracy": 0.785294,
            "f1": 0.799884,
            "f1_weighted": 0.784735
          },
          {
            "accuracy": 0.777941,
            "f1": 0.797169,
            "f1_weighted": 0.7814
          },
          {
            "accuracy": 0.768382,
            "f1": 0.789876,
            "f1_weighted": 0.771972
          },
          {
            "accuracy": 0.769118,
            "f1": 0.785626,
            "f1_weighted": 0.76946
          },
          {
            "accuracy": 0.772794,
            "f1": 0.78902,
            "f1_weighted": 0.773102
          }
        ],
        "main_score": 0.772794,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 42.98689365386963,
  "kg_co2_emissions": null
}