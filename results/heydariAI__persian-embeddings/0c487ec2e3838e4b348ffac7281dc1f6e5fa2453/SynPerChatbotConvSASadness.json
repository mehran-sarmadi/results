{
  "dataset_revision": "e9c678325565a5e4dadc43fd6693a8ccff1dd6b2",
  "task_name": "SynPerChatbotConvSASadness",
  "mteb_version": "1.38.34",
  "scores": {
    "test": [
      {
        "accuracy": 0.667647,
        "f1": 0.647159,
        "f1_weighted": 0.67521,
        "ap": 0.752143,
        "ap_weighted": 0.752143,
        "scores_per_experiment": [
          {
            "accuracy": 0.705882,
            "f1": 0.673774,
            "f1_weighted": 0.707889,
            "ap": 0.759358,
            "ap_weighted": 0.759358
          },
          {
            "accuracy": 0.598039,
            "f1": 0.580247,
            "f1_weighted": 0.609053,
            "ap": 0.714171,
            "ap_weighted": 0.714171
          },
          {
            "accuracy": 0.666667,
            "f1": 0.650403,
            "f1_weighted": 0.675538,
            "ap": 0.757265,
            "ap_weighted": 0.757265
          },
          {
            "accuracy": 0.686275,
            "f1": 0.664474,
            "f1_weighted": 0.692982,
            "ap": 0.760784,
            "ap_weighted": 0.760784
          },
          {
            "accuracy": 0.617647,
            "f1": 0.600723,
            "f1_weighted": 0.628124,
            "ap": 0.72656,
            "ap_weighted": 0.72656
          },
          {
            "accuracy": 0.666667,
            "f1": 0.639501,
            "f1_weighted": 0.672488,
            "ap": 0.742568,
            "ap_weighted": 0.742568
          },
          {
            "accuracy": 0.666667,
            "f1": 0.650403,
            "f1_weighted": 0.675538,
            "ap": 0.757265,
            "ap_weighted": 0.757265
          },
          {
            "accuracy": 0.676471,
            "f1": 0.66215,
            "f1_weighted": 0.685336,
            "ap": 0.766934,
            "ap_weighted": 0.766934
          },
          {
            "accuracy": 0.666667,
            "f1": 0.643503,
            "f1_weighted": 0.673794,
            "ap": 0.747304,
            "ap_weighted": 0.747304
          },
          {
            "accuracy": 0.72549,
            "f1": 0.706414,
            "f1_weighted": 0.73136,
            "ap": 0.789216,
            "ap_weighted": 0.789216
          }
        ],
        "main_score": 0.667647,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 18.52092170715332,
  "kg_co2_emissions": null
}