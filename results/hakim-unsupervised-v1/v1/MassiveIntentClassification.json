{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "validation": [
      {
        "accuracy": 0.683128,
        "f1": 0.649085,
        "f1_weighted": 0.68312,
        "scores_per_experiment": [
          {
            "accuracy": 0.677324,
            "f1": 0.644566,
            "f1_weighted": 0.678711
          },
          {
            "accuracy": 0.700443,
            "f1": 0.659235,
            "f1_weighted": 0.703043
          },
          {
            "accuracy": 0.666995,
            "f1": 0.639255,
            "f1_weighted": 0.663932
          },
          {
            "accuracy": 0.691589,
            "f1": 0.64681,
            "f1_weighted": 0.688224
          },
          {
            "accuracy": 0.687162,
            "f1": 0.658235,
            "f1_weighted": 0.686168
          },
          {
            "accuracy": 0.674373,
            "f1": 0.643269,
            "f1_weighted": 0.673266
          },
          {
            "accuracy": 0.705362,
            "f1": 0.667617,
            "f1_weighted": 0.706431
          },
          {
            "accuracy": 0.661092,
            "f1": 0.628669,
            "f1_weighted": 0.659865
          },
          {
            "accuracy": 0.673881,
            "f1": 0.645204,
            "f1_weighted": 0.678618
          },
          {
            "accuracy": 0.693064,
            "f1": 0.657994,
            "f1_weighted": 0.692945
          }
        ],
        "main_score": 0.683128,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.679422,
        "f1": 0.651405,
        "f1_weighted": 0.679956,
        "scores_per_experiment": [
          {
            "accuracy": 0.672495,
            "f1": 0.656964,
            "f1_weighted": 0.674725
          },
          {
            "accuracy": 0.682919,
            "f1": 0.643692,
            "f1_weighted": 0.686264
          },
          {
            "accuracy": 0.674512,
            "f1": 0.650108,
            "f1_weighted": 0.670645
          },
          {
            "accuracy": 0.704438,
            "f1": 0.668115,
            "f1_weighted": 0.700872
          },
          {
            "accuracy": 0.67922,
            "f1": 0.661106,
            "f1_weighted": 0.675912
          },
          {
            "accuracy": 0.670814,
            "f1": 0.643564,
            "f1_weighted": 0.670116
          },
          {
            "accuracy": 0.697377,
            "f1": 0.661454,
            "f1_weighted": 0.698136
          },
          {
            "accuracy": 0.660726,
            "f1": 0.640278,
            "f1_weighted": 0.662629
          },
          {
            "accuracy": 0.682582,
            "f1": 0.651495,
            "f1_weighted": 0.688536
          },
          {
            "accuracy": 0.669132,
            "f1": 0.63727,
            "f1_weighted": 0.671723
          }
        ],
        "main_score": 0.679422,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 12.201831579208374,
  "kg_co2_emissions": null
}