{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "validation": [
      {
        "accuracy": 0.685735,
        "f1": 0.618835,
        "f1_weighted": 0.675429,
        "scores_per_experiment": [
          {
            "accuracy": 0.691589,
            "f1": 0.627266,
            "f1_weighted": 0.680612
          },
          {
            "accuracy": 0.713232,
            "f1": 0.633803,
            "f1_weighted": 0.707959
          },
          {
            "accuracy": 0.695032,
            "f1": 0.630319,
            "f1_weighted": 0.682521
          },
          {
            "accuracy": 0.694048,
            "f1": 0.619836,
            "f1_weighted": 0.681099
          },
          {
            "accuracy": 0.691097,
            "f1": 0.615099,
            "f1_weighted": 0.678323
          },
          {
            "accuracy": 0.6606,
            "f1": 0.613949,
            "f1_weighted": 0.64839
          },
          {
            "accuracy": 0.701918,
            "f1": 0.642698,
            "f1_weighted": 0.693408
          },
          {
            "accuracy": 0.653714,
            "f1": 0.575947,
            "f1_weighted": 0.643094
          },
          {
            "accuracy": 0.66306,
            "f1": 0.598945,
            "f1_weighted": 0.650846
          },
          {
            "accuracy": 0.693064,
            "f1": 0.630489,
            "f1_weighted": 0.688036
          }
        ],
        "main_score": 0.685735,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.683625,
        "f1": 0.639273,
        "f1_weighted": 0.674158,
        "scores_per_experiment": [
          {
            "accuracy": 0.704775,
            "f1": 0.663902,
            "f1_weighted": 0.694542
          },
          {
            "accuracy": 0.707128,
            "f1": 0.650989,
            "f1_weighted": 0.702058
          },
          {
            "accuracy": 0.679892,
            "f1": 0.64448,
            "f1_weighted": 0.666041
          },
          {
            "accuracy": 0.690989,
            "f1": 0.633134,
            "f1_weighted": 0.680305
          },
          {
            "accuracy": 0.677875,
            "f1": 0.630455,
            "f1_weighted": 0.66194
          },
          {
            "accuracy": 0.660726,
            "f1": 0.632023,
            "f1_weighted": 0.652801
          },
          {
            "accuracy": 0.704438,
            "f1": 0.657802,
            "f1_weighted": 0.69506
          },
          {
            "accuracy": 0.664761,
            "f1": 0.61313,
            "f1_weighted": 0.654358
          },
          {
            "accuracy": 0.663753,
            "f1": 0.624659,
            "f1_weighted": 0.656706
          },
          {
            "accuracy": 0.68191,
            "f1": 0.64216,
            "f1_weighted": 0.677765
          }
        ],
        "main_score": 0.683625,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 11.331804275512695,
  "kg_co2_emissions": null
}