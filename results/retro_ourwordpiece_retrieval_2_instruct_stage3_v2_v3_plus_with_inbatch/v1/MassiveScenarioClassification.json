{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "validation": [
      {
        "accuracy": 0.831677,
        "f1": 0.822724,
        "f1_weighted": 0.831639,
        "scores_per_experiment": [
          {
            "accuracy": 0.842597,
            "f1": 0.831397,
            "f1_weighted": 0.841702
          },
          {
            "accuracy": 0.841613,
            "f1": 0.83302,
            "f1_weighted": 0.841481
          },
          {
            "accuracy": 0.823906,
            "f1": 0.815416,
            "f1_weighted": 0.824853
          },
          {
            "accuracy": 0.822922,
            "f1": 0.815164,
            "f1_weighted": 0.823187
          },
          {
            "accuracy": 0.842597,
            "f1": 0.834392,
            "f1_weighted": 0.8406
          },
          {
            "accuracy": 0.819479,
            "f1": 0.811547,
            "f1_weighted": 0.81726
          },
          {
            "accuracy": 0.815544,
            "f1": 0.807083,
            "f1_weighted": 0.820802
          },
          {
            "accuracy": 0.815544,
            "f1": 0.80931,
            "f1_weighted": 0.817375
          },
          {
            "accuracy": 0.850959,
            "f1": 0.840392,
            "f1_weighted": 0.848115
          },
          {
            "accuracy": 0.841613,
            "f1": 0.829515,
            "f1_weighted": 0.841012
          }
        ],
        "main_score": 0.831677,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.83117,
        "f1": 0.822284,
        "f1_weighted": 0.830809,
        "scores_per_experiment": [
          {
            "accuracy": 0.839274,
            "f1": 0.829021,
            "f1_weighted": 0.837136
          },
          {
            "accuracy": 0.843309,
            "f1": 0.830785,
            "f1_weighted": 0.842378
          },
          {
            "accuracy": 0.82616,
            "f1": 0.817062,
            "f1_weighted": 0.82657
          },
          {
            "accuracy": 0.823134,
            "f1": 0.816112,
            "f1_weighted": 0.824455
          },
          {
            "accuracy": 0.842636,
            "f1": 0.830633,
            "f1_weighted": 0.838048
          },
          {
            "accuracy": 0.825151,
            "f1": 0.816921,
            "f1_weighted": 0.820998
          },
          {
            "accuracy": 0.819771,
            "f1": 0.813652,
            "f1_weighted": 0.824608
          },
          {
            "accuracy": 0.814728,
            "f1": 0.809318,
            "f1_weighted": 0.81808
          },
          {
            "accuracy": 0.841627,
            "f1": 0.831059,
            "f1_weighted": 0.839809
          },
          {
            "accuracy": 0.835911,
            "f1": 0.828277,
            "f1_weighted": 0.836004
          }
        ],
        "main_score": 0.83117,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 6.2215235233306885,
  "kg_co2_emissions": null
}