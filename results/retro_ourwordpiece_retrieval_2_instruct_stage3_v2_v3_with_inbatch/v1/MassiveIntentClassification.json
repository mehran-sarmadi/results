{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "validation": [
      {
        "accuracy": 0.73876,
        "f1": 0.696342,
        "f1_weighted": 0.738046,
        "scores_per_experiment": [
          {
            "accuracy": 0.742253,
            "f1": 0.701065,
            "f1_weighted": 0.745098
          },
          {
            "accuracy": 0.750123,
            "f1": 0.708661,
            "f1_weighted": 0.749508
          },
          {
            "accuracy": 0.723069,
            "f1": 0.681113,
            "f1_weighted": 0.720667
          },
          {
            "accuracy": 0.756026,
            "f1": 0.702116,
            "f1_weighted": 0.752087
          },
          {
            "accuracy": 0.740777,
            "f1": 0.702457,
            "f1_weighted": 0.739116
          },
          {
            "accuracy": 0.739793,
            "f1": 0.708865,
            "f1_weighted": 0.734831
          },
          {
            "accuracy": 0.746188,
            "f1": 0.703946,
            "f1_weighted": 0.747596
          },
          {
            "accuracy": 0.724545,
            "f1": 0.675163,
            "f1_weighted": 0.724288
          },
          {
            "accuracy": 0.727496,
            "f1": 0.687392,
            "f1_weighted": 0.729962
          },
          {
            "accuracy": 0.737334,
            "f1": 0.692644,
            "f1_weighted": 0.737304
          }
        ],
        "main_score": 0.73876,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.733894,
        "f1": 0.695292,
        "f1_weighted": 0.735825,
        "scores_per_experiment": [
          {
            "accuracy": 0.741762,
            "f1": 0.710426,
            "f1_weighted": 0.745493
          },
          {
            "accuracy": 0.740417,
            "f1": 0.694266,
            "f1_weighted": 0.740502
          },
          {
            "accuracy": 0.726631,
            "f1": 0.681675,
            "f1_weighted": 0.724448
          },
          {
            "accuracy": 0.753531,
            "f1": 0.700544,
            "f1_weighted": 0.754392
          },
          {
            "accuracy": 0.728312,
            "f1": 0.696798,
            "f1_weighted": 0.727687
          },
          {
            "accuracy": 0.728985,
            "f1": 0.688788,
            "f1_weighted": 0.72769
          },
          {
            "accuracy": 0.738399,
            "f1": 0.702292,
            "f1_weighted": 0.742786
          },
          {
            "accuracy": 0.731002,
            "f1": 0.694878,
            "f1_weighted": 0.732718
          },
          {
            "accuracy": 0.729657,
            "f1": 0.694774,
            "f1_weighted": 0.737334
          },
          {
            "accuracy": 0.720242,
            "f1": 0.688478,
            "f1_weighted": 0.725199
          }
        ],
        "main_score": 0.733894,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 15.309236288070679,
  "kg_co2_emissions": null
}