{
  "dataset_revision": "1f403cfadb85004fbf7e2480334fffc4c999b4ab",
  "task_name": "SynPerChatbotConvSAToneChatbotClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "test": [
      {
        "accuracy": 0.985791,
        "f1": 0.968219,
        "f1_weighted": 0.98601,
        "scores_per_experiment": [
          {
            "accuracy": 0.985324,
            "f1": 0.96694,
            "f1_weighted": 0.985561
          },
          {
            "accuracy": 0.984656,
            "f1": 0.965639,
            "f1_weighted": 0.984947
          },
          {
            "accuracy": 0.984656,
            "f1": 0.965639,
            "f1_weighted": 0.984947
          },
          {
            "accuracy": 0.988659,
            "f1": 0.974348,
            "f1_weighted": 0.988811
          },
          {
            "accuracy": 0.982655,
            "f1": 0.961279,
            "f1_weighted": 0.983059
          },
          {
            "accuracy": 0.987325,
            "f1": 0.971711,
            "f1_weighted": 0.987378
          },
          {
            "accuracy": 0.986658,
            "f1": 0.97035,
            "f1_weighted": 0.986753
          },
          {
            "accuracy": 0.985324,
            "f1": 0.96694,
            "f1_weighted": 0.985561
          },
          {
            "accuracy": 0.983989,
            "f1": 0.964206,
            "f1_weighted": 0.984307
          },
          {
            "accuracy": 0.988659,
            "f1": 0.975134,
            "f1_weighted": 0.988772
          }
        ],
        "main_score": 0.985791,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 13.076907634735107,
  "kg_co2_emissions": null
}