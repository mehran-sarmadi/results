{
  "dataset_revision": "5cae68b7fc094cb2fa6890a464e4d836e8107f5e",
  "task_name": "SynPerChatbotConvSAAnger",
  "mteb_version": "1.34.14",
  "scores": {
    "test": [
      {
        "accuracy": 0.833415,
        "f1": 0.81614,
        "f1_weighted": 0.835599,
        "ap": 0.865034,
        "ap_weighted": 0.865034,
        "scores_per_experiment": [
          {
            "accuracy": 0.821951,
            "f1": 0.804342,
            "f1_weighted": 0.824671,
            "ap": 0.85798,
            "ap_weighted": 0.85798
          },
          {
            "accuracy": 0.843902,
            "f1": 0.826455,
            "f1_weighted": 0.845513,
            "ap": 0.870076,
            "ap_weighted": 0.870076
          },
          {
            "accuracy": 0.836585,
            "f1": 0.820423,
            "f1_weighted": 0.839082,
            "ap": 0.870245,
            "ap_weighted": 0.870245
          },
          {
            "accuracy": 0.839024,
            "f1": 0.819122,
            "f1_weighted": 0.839902,
            "ap": 0.860752,
            "ap_weighted": 0.860752
          },
          {
            "accuracy": 0.780488,
            "f1": 0.762095,
            "f1_weighted": 0.785005,
            "ap": 0.831855,
            "ap_weighted": 0.831855
          },
          {
            "accuracy": 0.878049,
            "f1": 0.862468,
            "f1_weighted": 0.8785,
            "ap": 0.891741,
            "ap_weighted": 0.891741
          },
          {
            "accuracy": 0.8,
            "f1": 0.781938,
            "f1_weighted": 0.803674,
            "ap": 0.844259,
            "ap_weighted": 0.844259
          },
          {
            "accuracy": 0.839024,
            "f1": 0.819773,
            "f1_weighted": 0.840174,
            "ap": 0.862483,
            "ap_weighted": 0.862483
          },
          {
            "accuracy": 0.843902,
            "f1": 0.832264,
            "f1_weighted": 0.847566,
            "ap": 0.890997,
            "ap_weighted": 0.890997
          },
          {
            "accuracy": 0.85122,
            "f1": 0.83252,
            "f1_weighted": 0.851902,
            "ap": 0.869955,
            "ap_weighted": 0.869955
          }
        ],
        "main_score": 0.833415,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 12.780153751373291,
  "kg_co2_emissions": null
}