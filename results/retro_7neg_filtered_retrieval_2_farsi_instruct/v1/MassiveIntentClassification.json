{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "validation": [
      {
        "accuracy": 0.654993,
        "f1": 0.6268,
        "f1_weighted": 0.658685,
        "scores_per_experiment": [
          {
            "accuracy": 0.658633,
            "f1": 0.634974,
            "f1_weighted": 0.662351
          },
          {
            "accuracy": 0.660108,
            "f1": 0.61785,
            "f1_weighted": 0.666879
          },
          {
            "accuracy": 0.653222,
            "f1": 0.620169,
            "f1_weighted": 0.656896
          },
          {
            "accuracy": 0.660108,
            "f1": 0.62603,
            "f1_weighted": 0.659045
          },
          {
            "accuracy": 0.65273,
            "f1": 0.636095,
            "f1_weighted": 0.658264
          },
          {
            "accuracy": 0.655189,
            "f1": 0.624534,
            "f1_weighted": 0.653855
          },
          {
            "accuracy": 0.680767,
            "f1": 0.655961,
            "f1_weighted": 0.681893
          },
          {
            "accuracy": 0.637482,
            "f1": 0.60438,
            "f1_weighted": 0.642012
          },
          {
            "accuracy": 0.64486,
            "f1": 0.630092,
            "f1_weighted": 0.654449
          },
          {
            "accuracy": 0.646827,
            "f1": 0.617921,
            "f1_weighted": 0.65121
          }
        ],
        "main_score": 0.654993,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.650504,
        "f1": 0.632193,
        "f1_weighted": 0.654779,
        "scores_per_experiment": [
          {
            "accuracy": 0.665098,
            "f1": 0.649676,
            "f1_weighted": 0.669063
          },
          {
            "accuracy": 0.653329,
            "f1": 0.625896,
            "f1_weighted": 0.659742
          },
          {
            "accuracy": 0.639206,
            "f1": 0.627047,
            "f1_weighted": 0.635339
          },
          {
            "accuracy": 0.659381,
            "f1": 0.636264,
            "f1_weighted": 0.664781
          },
          {
            "accuracy": 0.650303,
            "f1": 0.637983,
            "f1_weighted": 0.649407
          },
          {
            "accuracy": 0.65232,
            "f1": 0.624275,
            "f1_weighted": 0.653053
          },
          {
            "accuracy": 0.667451,
            "f1": 0.638342,
            "f1_weighted": 0.670694
          },
          {
            "accuracy": 0.635171,
            "f1": 0.626343,
            "f1_weighted": 0.643769
          },
          {
            "accuracy": 0.646268,
            "f1": 0.630194,
            "f1_weighted": 0.657129
          },
          {
            "accuracy": 0.636516,
            "f1": 0.625907,
            "f1_weighted": 0.644817
          }
        ],
        "main_score": 0.650504,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 18.316717386245728,
  "kg_co2_emissions": null
}