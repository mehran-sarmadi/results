{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "validation": [
      {
        "accuracy": 0.664781,
        "f1": 0.6,
        "f1_weighted": 0.656661,
        "scores_per_experiment": [
          {
            "accuracy": 0.672405,
            "f1": 0.615822,
            "f1_weighted": 0.670555
          },
          {
            "accuracy": 0.695032,
            "f1": 0.616945,
            "f1_weighted": 0.692705
          },
          {
            "accuracy": 0.682735,
            "f1": 0.614117,
            "f1_weighted": 0.675758
          },
          {
            "accuracy": 0.669454,
            "f1": 0.595927,
            "f1_weighted": 0.662259
          },
          {
            "accuracy": 0.673389,
            "f1": 0.591327,
            "f1_weighted": 0.663056
          },
          {
            "accuracy": 0.640433,
            "f1": 0.60028,
            "f1_weighted": 0.628122
          },
          {
            "accuracy": 0.676832,
            "f1": 0.616338,
            "f1_weighted": 0.663236
          },
          {
            "accuracy": 0.646335,
            "f1": 0.575575,
            "f1_weighted": 0.636992
          },
          {
            "accuracy": 0.642892,
            "f1": 0.577055,
            "f1_weighted": 0.63339
          },
          {
            "accuracy": 0.648303,
            "f1": 0.59661,
            "f1_weighted": 0.640536
          }
        ],
        "main_score": 0.664781,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.662508,
        "f1": 0.621593,
        "f1_weighted": 0.653608,
        "scores_per_experiment": [
          {
            "accuracy": 0.681574,
            "f1": 0.641786,
            "f1_weighted": 0.67844
          },
          {
            "accuracy": 0.677875,
            "f1": 0.623784,
            "f1_weighted": 0.673965
          },
          {
            "accuracy": 0.658709,
            "f1": 0.621914,
            "f1_weighted": 0.647042
          },
          {
            "accuracy": 0.667787,
            "f1": 0.62186,
            "f1_weighted": 0.66214
          },
          {
            "accuracy": 0.659381,
            "f1": 0.614055,
            "f1_weighted": 0.645302
          },
          {
            "accuracy": 0.644923,
            "f1": 0.619852,
            "f1_weighted": 0.634533
          },
          {
            "accuracy": 0.680565,
            "f1": 0.637317,
            "f1_weighted": 0.666417
          },
          {
            "accuracy": 0.652993,
            "f1": 0.609313,
            "f1_weighted": 0.641608
          },
          {
            "accuracy": 0.65501,
            "f1": 0.608752,
            "f1_weighted": 0.648183
          },
          {
            "accuracy": 0.646268,
            "f1": 0.617293,
            "f1_weighted": 0.638452
          }
        ],
        "main_score": 0.662508,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 10.329068422317505,
  "kg_co2_emissions": null
}