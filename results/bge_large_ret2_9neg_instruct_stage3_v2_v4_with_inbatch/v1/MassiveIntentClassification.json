{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "validation": [
      {
        "accuracy": 0.715002,
        "f1": 0.670559,
        "f1_weighted": 0.716518,
        "scores_per_experiment": [
          {
            "accuracy": 0.729956,
            "f1": 0.687293,
            "f1_weighted": 0.73375
          },
          {
            "accuracy": 0.735858,
            "f1": 0.66539,
            "f1_weighted": 0.737197
          },
          {
            "accuracy": 0.707329,
            "f1": 0.663778,
            "f1_weighted": 0.706518
          },
          {
            "accuracy": 0.723561,
            "f1": 0.667627,
            "f1_weighted": 0.721553
          },
          {
            "accuracy": 0.717167,
            "f1": 0.68502,
            "f1_weighted": 0.719836
          },
          {
            "accuracy": 0.719626,
            "f1": 0.686423,
            "f1_weighted": 0.720196
          },
          {
            "accuracy": 0.719134,
            "f1": 0.685809,
            "f1_weighted": 0.721994
          },
          {
            "accuracy": 0.693556,
            "f1": 0.648818,
            "f1_weighted": 0.693026
          },
          {
            "accuracy": 0.70241,
            "f1": 0.658324,
            "f1_weighted": 0.709131
          },
          {
            "accuracy": 0.701426,
            "f1": 0.657111,
            "f1_weighted": 0.701985
          }
        ],
        "main_score": 0.715002,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.701816,
        "f1": 0.65819,
        "f1_weighted": 0.705547,
        "scores_per_experiment": [
          {
            "accuracy": 0.705783,
            "f1": 0.666861,
            "f1_weighted": 0.710443
          },
          {
            "accuracy": 0.72495,
            "f1": 0.671407,
            "f1_weighted": 0.729028
          },
          {
            "accuracy": 0.69805,
            "f1": 0.651214,
            "f1_weighted": 0.698659
          },
          {
            "accuracy": 0.715871,
            "f1": 0.662318,
            "f1_weighted": 0.719832
          },
          {
            "accuracy": 0.697377,
            "f1": 0.660065,
            "f1_weighted": 0.696913
          },
          {
            "accuracy": 0.703766,
            "f1": 0.655925,
            "f1_weighted": 0.706929
          },
          {
            "accuracy": 0.713517,
            "f1": 0.673688,
            "f1_weighted": 0.7186
          },
          {
            "accuracy": 0.685272,
            "f1": 0.641416,
            "f1_weighted": 0.686378
          },
          {
            "accuracy": 0.685272,
            "f1": 0.640791,
            "f1_weighted": 0.692746
          },
          {
            "accuracy": 0.688299,
            "f1": 0.658214,
            "f1_weighted": 0.695942
          }
        ],
        "main_score": 0.701816,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 37.36633324623108,
  "kg_co2_emissions": null
}